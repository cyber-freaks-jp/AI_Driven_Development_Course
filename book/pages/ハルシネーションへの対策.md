# ハルシネーションへの対策

## ハルシネーションとは

**ハルシネーション（幻覚）**は、AIモデルが実際には存在しない情報を生成したり、誤った情報を確信を持って提示したりする現象です。これは、AI駆動開発において最も注意すべき課題の一つです。

### 具体例

```python
# 開発者がAIに質問
「Pythonの sort_array_fast() 関数の使い方を教えてください」

# AIの回答（ハルシネーション）
「sort_array_fast() は Python 3.10 で追加された組み込み関数です。
以下のように使用します：
result = sort_array_fast([3, 1, 2])
```

**問題**：`sort_array_fast()` という関数は存在しません。AIが作り出した架空の情報です。

## ハルシネーションの3つの種類

### 1. 事実誤認型ハルシネーション

存在しない情報や誤った情報を生成します。

**例**：
- 存在しないライブラリやAPIを提案
- 誤ったバージョン情報
- 架空の引用や参考文献

```
「express-super-validator というライブラリを使うと簡単です」
→ そんなライブラリは存在しない
```

### 2. 論理矛盾型ハルシネーション

矛盾する情報や一貫性のない説明を生成します。

**例**：
```
AIの回答：
「このAPIは同期的に動作します。
 そのため、await キーワードを使用して非同期に呼び出す必要があります。」

→ 同期なのに非同期？矛盾している
```

### 3. 過剰確信型ハルシネーション

不確かな情報を確実であるかのように提示します。

**例**：
```
「このコードは100%安全です。セキュリティの問題は一切ありません。」

→ 実際にはSQLインジェクション脆弱性がある
```

## なぜハルシネーションが発生するのか

### 原因1：学習データの制限

- 学習データに含まれない情報は推測で補完する
- 最新情報（学習後に登場した技術）は知らない
- データの品質が低い場合、誤った情報を学習

### 原因2：プロンプトが曖昧

```
❌ 悪い例：「認証を実装して」
→ AIは具体的な情報がないため、推測で回答
→ ハルシネーションが発生しやすい

✅ 良い例：「Node.js + Express で JWT認証を実装してください。
            bcryptでパスワードをハッシュ化し、
            トークンの有効期限は24時間に設定してください。」
→ 具体的な情報があるため、正確な回答が得られやすい
```

### 原因3：AIの特性

- 「分からない」と言うのが苦手
- それっぽい回答を生成してしまう
- 自信満々に間違った情報を提示する

## ハルシネーションの検出方法

### 検出基準1：情報の検証可能性

**チェック項目**：
- [ ] 公式ドキュメントで確認できるか
- [ ] 実際に動作するか
- [ ] 他の信頼できる情報源と一致するか

**実践例**：

```python
# AIが提案したコード
import obscure_library
result = obscure_library.magic_function()

# 検証
# 1. ライブラリが実在するか確認
pip search obscure_library  # → 見つからない

# 2. 公式ドキュメントを確認
# → 存在しない

# 結論：ハルシネーション
```

### 検出基準2：論理的一貫性

**チェック項目**：
- [ ] 説明に矛盾がないか
- [ ] 前後の文脈と一致しているか
- [ ] 専門知識と照らし合わせて正しいか

**実践例**：

```
AIの回答：
「この関数は O(1) の時間計算量で動作します。
 すべての要素をループで処理します。」

検証：
- 全要素をループ → O(n)
- O(1) とは矛盾

結論：論理矛盾型ハルシネーション
```

### 検出基準3：過剰な確信

**チェック項目**：
- [ ] 断定的すぎないか
- [ ] 代替案や注意点が提示されているか
- [ ] 不確実性について言及があるか

**実践例**：

```
❌ ハルシネーションの疑いあり：
「このコードは完璧です。問題は一切ありません。」

✅ 信頼できる回答：
「このコードは一般的なケースでは動作しますが、
 以下の点に注意が必要です：
 - 入力値が null の場合のエラーハンドリング
 - 大量データでのパフォーマンス
 推奨事項として、以下のテストを追加してください...」
```

## ハルシネーションへの対策

### 対策1：具体的なプロンプトを書く

```
❌ 悪い例：
「データベースを設計して」

✅ 良い例：
「@Docs/要件定義書.md をもとに、
 PostgreSQL でユーザー管理システムのデータベースを設計してください。
 以下のテーブルが必要です：
 - users（id, email, password_hash, created_at）
 - profiles（user_id, name, bio）
 ER図をMermaid形式で作成し、各テーブルの説明を追加してください。」
```

### 対策2：「不確かな場合は明示して」と指示

```
プロンプト例：
「Node.jsで画像処理をしたいです。
 推奨ライブラリを教えてください。

 ⚠️ 重要：
 - 確実な情報のみを提供してください
 - 不確かな情報は「確認が必要」と明示してください
 - 存在しないライブラリは提案しないでください」
```

### 対策3：検証を依頼する

```
プロンプト例：
「先ほど提案された sort_array_fast() 関数について、
 以下を確認してください：
 1. この関数は本当にPythonの組み込み関数ですか？
 2. 公式ドキュメントのURLを提示できますか？
 3. 代替案はありますか？」
```

### 対策4：段階的に質問する

```
ステップ1：「Pythonで配列をソートする方法を教えてください」
ステップ2：「sorted() 関数の使い方を詳しく教えてください」
ステップ3：「カスタムソートキーを使う例を教えてください」

→ 段階的に質問することで、AIが推測する範囲を減らす
```

### 対策5：複数の情報源で確認

```
1. AIに質問
2. 公式ドキュメントで確認
3. 実際に動作確認
4. コミュニティ（Stack Overflow等）で確認
```

## 実践的な検出・対策システム

### チェックリスト

```markdown
## AIの回答検証チェックリスト

### 事実確認
- [ ] 公式ドキュメントで確認した
- [ ] 実際に動作確認した
- [ ] バージョン情報が正しい
- [ ] ライブラリ/関数が実在する

### 論理確認
- [ ] 説明に矛盾がない
- [ ] 前後の文脈と一致
- [ ] 専門知識と照合済み

### 確信度確認
- [ ] 不確実性が適切に表明されている
- [ ] 代替案が提示されている
- [ ] 注意点が明記されている

### 総合判断
- [ ] 問題なし → 採用
- [ ] 疑わしい → 再確認
- [ ] ハルシネーション → 却下
```

### 自動検証スクリプト

```python
def verify_ai_suggestion(code: str, explanation: str) -> dict:
    results = {
        "syntax_valid": False,
        "imports_exist": False,
        "logic_consistent": False,
        "warnings": []
    }

    # 構文チェック
    try:
        compile(code, '<string>', 'exec')
        results["syntax_valid"] = True
    except SyntaxError as e:
        results["warnings"].append(f"構文エラー: {e}")

    # インポート確認
    imports = extract_imports(code)
    for imp in imports:
        if not module_exists(imp):
            results["warnings"].append(f"存在しないモジュール: {imp}")
        else:
            results["imports_exist"] = True

    # 論理チェック（説明とコードの一致）
    if explanation_matches_code(explanation, code):
        results["logic_consistent"] = True
    else:
        results["warnings"].append("説明とコードが一致しません")

    return results
```

## まとめ

| 対策 | 内容 |
|-----|------|
| プロンプトを具体的に | 曖昧な指示を避ける |
| 不確実性の明示を求める | 「分からない場合は明示して」 |
| 検証を依頼 | 情報源の提示を求める |
| 段階的に質問 | 一度に複雑な質問をしない |
| 複数の情報源で確認 | 公式ドキュメント、動作確認 |
| チェックリスト活用 | 見落としを防ぐ |

**重要な原則**：

1. **AIを過信しない**：AIも間違える
2. **必ず検証する**：公式ドキュメント、動作確認
3. **断定的な回答に注意**：「100%安全」は疑う
4. **存在確認**：ライブラリ、関数、API が実在するか確認
5. **論理矛盾をチェック**：説明に矛盾がないか確認

ハルシネーションは完全には防げませんが、適切な対策と検証により、そのリスクを大幅に削減できます。
